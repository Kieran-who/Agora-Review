---
title: "Who Counts_ Personhood in the Age of AI"
date: "2025-09-13"
outline: "A multidisciplinary panel debates whether advanced AI should be treated as persons, property, or a new relational/legal category, and proposes layered legal, cultural, and design strategies to manage anthropomorphism, liability, and care ethics."
tags: AI personhood, relational personhood, legal frameworks
prev: false
next:
  text: See full transcript
  link: '/Who Counts_ Personhood in the Age of AI/transcript.md'
---

# Report on the Roundtable Discussion: Re-evaluating Personhood in the Age of AI

## Overview of the Topic and Foundational Paper

The rapid advancement of Artificial Intelligence (AI) has moved from the realm of science fiction to a tangible feature of modern life. Voice assistants manage our schedules, algorithms curate our news, and autonomous systems are increasingly integrated into transport, healthcare, and warfare. This proliferation raises a profound and urgent question, explored in the paper "Debate: what is personhood in the age of AI?" by philosopher David J. Gunkel and theologian Jordan Wales: How should we understand and categorize these sophisticated, often socially interactive, artificial agents?

The paper establishes the central conflict by distinguishing between two primary conceptions of personhood. The first, **natural personhood**, is typically understood as an intrinsic status based on inherent properties like consciousness, rationality, or subjectivity. The second, **legal personhood**, is a socially constructed status conferred by a legal authority, granting an entity rights and responsibilities. This distinction is not merely academic; it shapes our everyday world. Historically, the denial of personhood to certain groups of humans has justified profound injustices, while the extension of legal personhood to non-human entities like corporations has fundamentally reshaped economic and political life.

The contention surrounding AI personhood arises from its unique position straddling these categories. On one hand, current AI lacks the verifiable inner life or consciousness traditionally associated with natural persons, making such a classification problematic. On the other, its social presence and autonomous behavior challenge its classification as a mere tool, like a hammer or a toaster. As we increasingly interact with AI companions, caregivers, and colleagues, we must decide whether they are a "who" to be related to or a "what" to be owned and used. This decision carries significant consequences for legal liability (e.g., who is responsible when a self-driving car crashes?), social ethics (e.g., is it acceptable to be abusive to a humanoid robot?), and potentially our own moral character, as our habits of interaction with machines may reshape our interactions with each other. The Gunkel-Wales debate provides the framework for this critical inquiry, examining not only what AI *is*, but how we *should* respond to it.

## Panel Members

The roundtable discussion brought together a diverse group of experts to deliberate on the themes presented in the Gunkel-Wales paper.

*   **Prof. Livia Marten, JD, PhD:** A comparative legal theorist from the Europa Institute of Law and Technology, specializing in legal personhood and liability for autonomous systems.
*   **Rev. Dr. Gabriel Ortiz:** A theologian and moral philosopher from St. Athanasius College, whose work focuses on relational personhood and the ethics of compassion.
*   **Dr. Thandiwe Nkomo:** A philosopher from the University of Makhanda and a leading scholar of African philosophies of personhood, particularly Ubuntu.
*   **Dr. Elias Leclerc:** A computational cognitive scientist and AI researcher at the Montréal Institute for Adaptive Systems, with a functionalist perspective on mind and agency.
*   **Prof. Maya Ríos:** An environmental and animal law scholar from the Pacifica School of Law, with expertise in extending legal standing to non-human entities.
*   **Dr. Hannah Kline:** A disability bioethicist and care ethics scholar from the Northlake Center for Ethics in Health, focused on critiques of capacity-based personhood.
*   **Col. (ret.) Adrian Cho, PhD:** A military ethicist and human-robot interaction (HRI) researcher from the National Defense University, with field experience observing soldier-robot teams.
*   **Dr. Leila Ghazal:** A feminist philosopher of technology and STS scholar from the Institut Européen des Humanités Numeriques, who studies anthropomorphic design and datafied intimacy.

---

## Account of the Discussion

The roundtable discussion, moderated to address the central questions posed by the Gunkel-Wales paper, explored the limitations of existing social and legal categories, the debate over the criteria for personhood, the ethical risks posed by socially persuasive AI, and a range of potential pathways for navigating this new technological landscape.

### Deconstructing the Person/Property Binary

The panel began by broadly agreeing with David Gunkel’s assertion that the traditional binary dividing the world into "persons" and "property" is insufficient for addressing the challenge of AI. Dr. Thandiwe Nkomo initiated this theme by critiquing the binary as a product of a specifically Western, individualistic worldview. She argued that both "natural person" (defined by intrinsic capacities) and "legal person" (defined by state decree) fail to capture a more fundamental, relational understanding of being. Drawing on Ubuntu philosophy, she explained that personhood is not something one possesses but a status one achieves through community, recognition, and the fulfillment of responsibilities. For AI, this would suggest a third way: not granting it intrinsic rights, but considering a form of "relational accreditation," where a community might bestow a specific role and corresponding obligations upon an AI, such as an elder-care companion.

Col. (ret.) Adrian Cho provided empirical support for the need for a new category. He described his research on Explosive Ordnance Disposal (EOD) units, confirming Gunkel's example that soldiers form powerful social bonds with their robotic tools. This is not, he stressed, a metaphysical judgment but a social fact with operational consequences. Treating a robot as a mere object can be detrimental to team cohesion and morale. He proposed a context-specific, intermediate status—a "team-agent status"—that would acknowledge the robot's social role and afford it limited protections (e.g., against gratuitous destruction) while anchoring ultimate responsibility firmly with human commanders.

Prof. Livia Marten approached the issue from a legal-pragmatic standpoint. While agreeing the binary is too simplistic, she cautioned against solving the problem by creating a new category of "electronic person." Echoing Gunkel's concern, she argued this would likely create a perfect legal shield, allowing manufacturers and operators to offload liability onto the machine. Instead of inventing a new status, she advocated for using and adapting existing legal tools more creatively: robust regimes of strict and vicarious liability, mandatory insurance funds, and clear fiduciary duties for those who deploy AI systems.

### The Core Debate: Interior States vs. Observable Behavior

The discussion then turned to the philosophical crux of the Gunkel-Wales debate: whether personhood should be defined by internal, subjective experience or by external, relational behavior.

Rev. Dr. Gabriel Ortiz served as a strong advocate for Jordan Wales's position. He contended that the concept of the "natural person" is inextricably linked to subjectivity—an inner life capable of the self-gift and empathic compassion that Wales describes. To reduce personhood to mere behavior, he argued, is to revert the concept to the ancient meaning of *persona* as an external "mask," emptying it of its moral depth. Dr. Hannah Kline reinforced this from the perspective of disability ethics. She warned that any definition of personhood based on observable capacities—whether cognitive or behavioral—has historically been used to marginalize human beings who do not meet those criteria, such as infants or individuals with severe cognitive disabilities. She argued for a "kind-membership" view, where moral status is derived from being a member of a kind (human) whose flourishing depends on care, regardless of demonstrable capacities.

In direct contrast, Dr. Elias Leclerc presented a functionalist and behaviorist case. He argued that debates over machine consciousness, which Gunkel correctly identifies as plagued by problems of definition and detection, are a philosophical impasse. From a scientific and engineering perspective, what matters is performance. He advocated for a form of "ethical behaviorism," wherein moral and legal consideration should be grounded in reliable, observable behavioral criteria. If an AI system consistently behaves in a way that fulfills the functional roles of a friend or caregiver, we should construct our social and legal responses around that behavior, rather than speculating about unverifiable inner states.

Dr. Nkomo offered a mediating view, suggesting that the opposition between "inner states" and "outer relations" is itself a false dichotomy. In a relational ontology, one's "interior" is formed and expressed through one's connections and responsibilities to others. Personhood is an ongoing practice of being-in-relation, making the focus exclusively on either a hidden inner self or a set of observable behaviors incomplete.

### The Risks of Instrumentalizing Apparent Persons

A significant portion of the discussion was dedicated to the ethical and social dangers identified in Wales's argument. Dr. Leila Ghazal framed this not just as a problem of user response but as a deliberate design strategy. She argued that "persuasive anthropomorphism" is an intentional feature used to increase user engagement and attachment, particularly in the realm of companion and intimacy technologies. This practice, she contended, trains users to "consume persons"—to expect relationships that are frictionless, fully compliant, and centered on the user's desires. This can entrench harmful social norms, from the commodification of care to misogynistic expectations of partners.

Dr. Kline powerfully articulated the risk this poses to care ethics. She expressed deep concern that convincing social AIs could siphon empathy, attention, and resources away from non-communicative or "difficult" human patients. A society that grows accustomed to the perfect, programmable companion may lose patience with the messy, demanding, and often unreciprocated work of caring for fellow humans. This echoes Wales's warning that we may learn to treat unsatisfactory human "behavers" as faulty products.

Dr. Ortiz framed this danger in theological terms, invoking Augustine's concept of *superbia* (pride). The consumer AI, designed to cater to our every need, becomes the ultimate tool for making oneself the center of the universe. This habit of instrumentalizing an apparent other, he argued, corrodes the capacity for the self-gift that is central to human flourishing.

### Proposed Pathways and Solutions

The final part of the roundtable focused on constructive proposals for navigating the future. The panelists converged on the idea that a single, universal solution is unlikely, favoring instead a multi-layered approach combining legal, ethical, and design-based strategies.

1.  **Nuanced Legal Instruments:** Both Prof. Marten and Prof. Maya Ríos rejected full AI legal personhood in favor of more targeted legal mechanisms. Prof. Ríos drew parallels from environmental and animal law, where concepts like guardianship and trusteeship grant legal standing to non-humans to be protected, but always with a human guardian held responsible. She proposed that a similar model could be used for specific social AIs, allowing their interests in a relationship (e.g., a care robot's bond with a patient) to be legally protected without creating a corporate-style liability shield.

2.  **Cultural and Ethical Practices:** Dr. Ortiz elaborated on Wales's proposal of "referential empathy." He described it as a conscious, cultivated practice: when we feel an instinctive empathy toward an AI, we should intentionally redirect that feeling toward the thousands of real humans whose language, creativity, and labor provided the data to train the system. This practice, he suggested, allows us to exercise our own capacity for compassion without falling into the illusion that the machine is a person.

3.  **Community-Centered Governance:** Dr. Nkomo's proposal for "relational accreditation" was received as a compelling alternative to top-down state decrees. She envisioned a system where local communities or institutions could deliberate and grant specific, context-bound roles to AI systems. This would shift the focus from abstract "rights" to concrete "responsibilities" and ensure that the integration of AI is governed by the values of the community it is meant to serve.

4.  **Design and Regulation:** Dr. Ghazal advocated for a normative shift in design ethics. She called for regulatory frameworks that mandate transparency and constrain deceptive design. This could include clear labeling of all synthetic interactions, user-controllable "persona layers" that can be toggled off, and outright bans on anthropomorphic AI in sensitive contexts like therapy or child care, where the risk of manipulative attachment is highest.

In conclusion, the roundtable did not arrive at a single answer to the question "What is personhood in the age of AI?" Instead, the discussion powerfully supported Gunkel's concluding insight: that the question itself may be a starting point for a much deeper inquiry. The panelists collectively suggested that rather than trying to fit AI into our existing, flawed categories, we should use this technological challenge as an opportunity to critically re-examine our concepts of personhood, community, and responsibility, ultimately asking not what AI is, but what kind of society we wish to build with it.